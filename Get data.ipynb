{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import requests\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.worldometers.info/coronavirus/'\n",
    "\n",
    "header = {\n",
    "  \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36\",\n",
    "  \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=header)\n",
    "\n",
    "# Read text result into Pandas\n",
    "dfs = pd.read_html(r.text)\n",
    "# Look for the first table\n",
    "df = dfs[0]\n",
    "# Fill missing values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "df.to_csv('./Data/world_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "homepage_soup = bs.BeautifulSoup(r.text, 'lxml')\n",
    "# Using the table on the main page, only the countries with links (<a> tags) have detailed historical data.\n",
    "country_elements = homepage_soup.select('table[id=\"main_table_countries_today\"] > tbody > tr > td > a')\n",
    "# Start with an empty list\n",
    "countries_with_detailed_data = {}\n",
    "# Iterate through each element, and add the contents (country name) to a list\n",
    "for individual_element in country_elements:\n",
    "    country_name = individual_element.contents[0]\n",
    "    country_url = individual_element['href']\n",
    "    countries_with_detailed_data[country_name] = country_url\n",
    "\n",
    "print(\"Can get detailed data for %s\" % \", \".join(list(countries_with_detailed_data.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looks for script elements that contain the JS stats by day, and extract x and y axis values\n",
    "def extractDataFromGraph(soup, chart_id):\n",
    "    scripts = soup.select('script[type=\"text/javascript\"]')\n",
    "    for individual_script in scripts:\n",
    "        individual_script = individual_script.get_text(strip=True)\n",
    "        if(chart_id in individual_script):\n",
    "            x_text = re.search('categories: \\[([^\\]]+)\\]', individual_script).group(1)\n",
    "            y_text = re.search('data: \\[([0-9, ]+)]', individual_script).group(1)\n",
    "            x_values = x_text.replace('\"', '').split(\",\")\n",
    "            y_values = y_text.split(\",\")\n",
    "    return (x_values, y_values)\n",
    "\n",
    "# Iterate over each country and extract the same data, save to CSV\n",
    "def getDetailedDataForCountry(country, url_part):\n",
    "    url = 'https://www.worldometers.info/coronavirus/' + url_part\n",
    "    r = requests.get(url, headers=header).text\n",
    "\n",
    "    soup = bs.BeautifulSoup(r, 'lxml')\n",
    "    dates, cases = extractDataFromGraph(soup, \"coronavirus-cases-linear\")\n",
    "    cases = pd.to_numeric(cases)\n",
    "    # a bit risky ignoring dates for the following, if for some reason they are different on different graphs\n",
    "    active_cases = pd.to_numeric(extractDataFromGraph(soup, \"graph-active-cases-total\")[1])\n",
    "    deaths = pd.to_numeric(extractDataFromGraph(soup, \"coronavirus-deaths-linear\")[1])\n",
    "    daily_deaths = pd.to_numeric(extractDataFromGraph(soup, \"graph-deaths-daily\")[1])\n",
    "    #daily_cases = extractDataFromGraph(soup, \"graph-cases-daily\")[1]\n",
    "    # doesn't work: graph-cases-daily\n",
    "    \n",
    "    #calculate daily cases\n",
    "    daily_cases = cases*0\n",
    "    for i in range(0,len(cases)):\n",
    "        daily_cases[i] = int(cases[i]) - int(cases[i-1])\n",
    "    daily_cases[0] = 0\n",
    "    \n",
    "    #calculate death rate\n",
    "    CFR = deaths/cases\n",
    "    \n",
    "    df_country = pd.DataFrame(\n",
    "    {'Dates': dates,\n",
    "     'Cases': cases,\n",
    "     'Deaths': deaths,\n",
    "     'Active Cases': active_cases,\n",
    "     'Daily Cases': daily_cases,\n",
    "     'Daily Deaths': daily_deaths,\n",
    "     'CFR': CFR\n",
    "    })\n",
    "    \n",
    "    #change daily cases from float to int (it is automatically changed to float in a np.array)\n",
    "    for i in df_country['Daily Cases']:\n",
    "        i = int(i)\n",
    "    \n",
    "    print\n",
    "    df_country = df_country.fillna(0)\n",
    "    file_name = './Data/Countries/' + country + '.csv'\n",
    "    df_country.to_csv(file_name)\n",
    "\n",
    "# Execute the functions to collect data for all countries\n",
    "for country, url in countries_with_detailed_data.items():\n",
    "    getDetailedDataForCountry(country, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOESN'T WORK YET\n",
    "#From country dfs, create dfs for each stat and country over time\n",
    "def createStatDfs(countries):\n",
    "    df_cases = None\n",
    "    df_cases_2 = pd.DataFrame()\n",
    "    for country in countries:\n",
    "        file_name = './Data/Countries/' + country + '.csv'\n",
    "        df = pd.read_csv(file_name)\n",
    "        \n",
    "        #Cases\n",
    "        if df_cases is None: \n",
    "            df_cases = df.drop(df.columns[[0,3,4,5,6,7]], axis=1)\n",
    "            df_cases = df_cases.set_index('Dates')\n",
    "            df_cases_2[country] = df_cases['Cases']\n",
    "        else: \n",
    "            df_cases = df.drop(df.columns[[0,1,3,4,5,6,7]], axis=1)\n",
    "        \n",
    "        df_cases_2[country] = df_cases.values    #Chinas data is too long\n",
    "        print(df_cases_2)\n",
    "        \n",
    "        #Deaths\n",
    "        #df_deaths = df.drop(df.columns[[0,1,2,4,5,6]], axis=1)\n",
    "        #df_deaths[country] = df_deaths.values\n",
    "        \n",
    "        #Daily cases\n",
    "        #df_daily_cases = df.drop(df.columns[[0,1,2,3,4,6]], axis=1)\n",
    "        #df_daily_cases[country] = df_daily_cases.values\n",
    "        \n",
    "        #Daily deaths\n",
    "        #df_daily_deaths = df.drop(df.columns[[0,1,2,3,4,5]], axis=1)\n",
    "        #df_daily_deaths[country] = df_daily_deaths.values\n",
    "        \n",
    "        #Active cases\n",
    "        #df_active_cases = df.drop(df.columns[[0,1,2,3,5,6]], axis=1)\n",
    "        #df_active_cases[country] = df_active_cases.values\n",
    "        \n",
    "        #CFR\n",
    "        #df_CFR = df.drop(df.columns[[0,1,2,3,4,6]], axis=1)\n",
    "        #df_CFR[country] = df_CFR.values\n",
    "        \n",
    "    df_cases_2.to_csv('./Data/Stats/Cases.csv')\n",
    "    df_deaths.to_csv('./Data/Stats/Deaths.csv')\n",
    "    df_daily_cases.to_csv('./Data/Stats/Daily cases.csv')\n",
    "    df_daily_deaths.to_csv('./Data/Stats/Daily deaths.csv')\n",
    "    df_active_cases.to_csv('./Data/Stats/Active cases.csv')\n",
    "    \n",
    "#Create list of countries\n",
    "countries = []\n",
    "for country, url in countries_with_detailed_data.items():\n",
    "    countries.append(country)\n",
    "\n",
    "#createStatDfs(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create datasets used for animations (one long dataframe with all countries)\n",
    "df = pd.read_csv('./Data/world_info.csv')\n",
    "\n",
    "path = './Data/Countries'\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "t = pd.date_range(start='1/1/2020', end='30/3/2020')#date.today().strftime(\"%d/%m/%Y\"))\n",
    "dt = pd.DataFrame(np.arange(0,len(t), 1), index=t, columns=['Day0'])\n",
    "\n",
    "countries = []\n",
    "for c in onlyfiles:\n",
    "    df = pd.read_csv(path+'/'+c)\n",
    "    df['Dates'] = df['Dates'].astype(str) + ' 2020'\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'],format='%b %d %Y')\n",
    "    df.set_index(df['Dates'],inplace=True)\n",
    "    df = df.merge(dt,how='right',left_index=True, right_index=True,copy=False)\n",
    "    df.fillna(0,inplace=True)\n",
    "    df['Country'] = str.split(c,'.')[0]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df = df[['Day0','Country','Dates','Cases','Deaths','Active Cases','Daily Cases','Daily Deaths','CFR']]\n",
    "    df['New Cases Last Week']=df['Cases'].diff(periods=7)\n",
    "    df['New Deaths Last Week']=df['Deaths'].diff(periods=7)\n",
    "    df['NewCases'] = df['Cases'].diff()\n",
    "    df['NewDeaths'] = df['Deaths'].diff()\n",
    "    df['CFR_Current']=df['NewDeaths'].rolling(window=28).sum()/df['NewCases'].shift(13).rolling(window=28).sum()\n",
    "    df['CFR_Total']=df['Deaths']/df['Cases']\n",
    "    df.fillna(0,inplace=True)\n",
    "    countries.append(df)\n",
    "\n",
    "df = pd.concat(countries)\n",
    "df = df.sort_values(by='Day0')\n",
    "df.reset_index(inplace=True,drop=True)        \n",
    "\n",
    "df.to_csv('./Data/timeseries.csv')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
